from sklearn.cluster import KMeans
import numpy as np
import torch, os, sys
from sklearn.metrics import roc_curve, precision_recall_curve, f1_score
from transformers import BertTokenizer, BertModel
import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
sys.path.append(os.path.abspath("./AI"))
from identification_models import (
    get_KLUE_BERT_model,
    get_KoBERT_model,
    get_KoELECTRA_model,
    get_KLUE_Roberta_model,
    get_KoSBERT_model,
    get_XLMRoberta_model,
    train_model
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def find_threshold(model, train_loader=None, val_loader=None, use_train=False, device="cuda"):
    model.eval()
    y_probs, y_true = np.array([]), np.array([])
    def collect_predictions(loader):
        with torch.no_grad():
            for X_batch, mask_batch, y_batch in loader:
                X_batch, mask_batch, y_batch = X_batch.to(device), mask_batch.to(device), y_batch.to(device)
                outputs = model(X_batch, mask_batch)
                y_probs.extend(outputs.cpu().numpy().flatten())  # í™•ë¥ ê°’ ì €ì¥
                y_true.extend(y_batch.cpu().numpy().flatten())  # ì‹¤ì œ ë ˆì´ë¸” ì €ì¥

    # âœ… Train ë°ì´í„° í¬í•¨ ì—¬ë¶€ ì„ íƒ
    if use_train and train_loader:
        collect_predictions(train_loader)
    if val_loader:
        collect_predictions(val_loader)

    y_probs = np.array(y_probs)
    y_true = np.array(y_true)

    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
    kmeans.fit(y_probs.reshape(-1, 1))
    cluster_centers = np.sort(kmeans.cluster_centers_.flatten())  # êµ°ì§‘ ì¤‘ì‹¬ ì •ë ¬
    optimal_threshold = np.mean(cluster_centers)  # ë‘ êµ°ì§‘ ì¤‘ì‹¬ì˜ ì¤‘ê°„ê°’

    print(f"ğŸ“Œ ìµœì  ì„ê³„ê°’: {optimal_threshold:.4f}")
    return optimal_threshold

##########################################################################################################################################################################################
##########################################################################################################################################################################################
##########################################################################################################################################################################################
##########################################################################################################################################################################################

import numpy as np
import torch
from sklearn.cluster import KMeans
from sklearn.metrics import roc_curve, precision_recall_curve, f1_score

# (Optional) ê³µí†µ: ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜ì§‘ í•¨ìˆ˜
def collect_predictions(model, loader, device):
    """
    ì£¼ì–´ì§„ ë°ì´í„° ë¡œë”ì—ì„œ ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ ë ˆì´ë¸”ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    model.eval()
    y_probs, y_true = np.array([]), np.array([])  # ë¹ˆ NumPy ë°°ì—´ë¡œ ì´ˆê¸°í™”


    with torch.no_grad():
        for X_batch, mask_batch, y_batch in loader:
            X_batch = X_batch.to(device)
            mask_batch = mask_batch.to(device)
            y_batch = y_batch.to(device)

            outputs = model(X_batch, mask_batch)
            probs = outputs.cpu().numpy().flatten()
            labels = y_batch.cpu().numpy().flatten()

            y_probs = np.concatenate([y_probs, probs]) if y_probs.size else probs
            y_true = np.concatenate([y_true, labels]) if y_true.size else labels

    return y_probs, y_true


#####################################################################
# 1. KMeans ê¸°ë°˜ ì„ê³„ê°’
def find_threshold_kmeans(model, train_loader=None, val_loader=None, use_train=False, device="cuda"):
    """
    KMeansë¥¼ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡ í™•ë¥ ì˜ ë‘ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì˜ í‰ê· ì„ ì„ê³„ê°’ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    """
    y_probs, y_true = np.array([]), np.array([])
    if use_train and train_loader is not None:
        probs, labels = collect_predictions(model, train_loader, device)
        y_probs = np.concatenate([y_probs, probs]) if y_probs.size else probs
        y_true = np.concatenate([y_true, labels]) if y_true.size else labels
    if val_loader is not None:
        probs, labels = collect_predictions(model, val_loader, device)
        y_probs = np.concatenate([y_probs, probs]) if y_probs.size else probs
        y_true = np.concatenate([y_true, labels]) if y_true.size else labels

    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
    kmeans.fit(y_probs.reshape(-1, 1))
    cluster_centers = np.sort(kmeans.cluster_centers_.flatten())
    optimal_threshold = np.mean(cluster_centers)
    print(f"[KMeans] Optimal threshold: {optimal_threshold:.4f}")
    return optimal_threshold

#####################################################################
# 2. ROC Curve ê¸°ë°˜ ì„ê³„ê°’ (Youden's J í†µê³„ëŸ‰)
def find_threshold_roc(model, train_loader=None, val_loader=None, use_train=False, device="cuda"):
    """
    ROC Curveë¥¼ ì´ìš©í•˜ì—¬ Youden's J (tpr - fpr)ê°€ ìµœëŒ€ê°€ ë˜ëŠ” ì„ê³„ê°’ì„ ì„ íƒí•©ë‹ˆë‹¤.
    """
    y_probs, y_true = np.array([]), np.array([])
    if use_train and train_loader is not None:
        probs, labels = collect_predictions(model, train_loader, device)
        y_probs = np.concatenate([y_probs, probs]) if y_probs.size else probs
        y_true = np.concatenate([y_true, labels]) if y_true.size else labels
    if val_loader is not None:
        probs, labels = collect_predictions(model, val_loader, device)
        y_probs = np.concatenate([y_probs, probs]) if y_probs.size else probs
        y_true = np.concatenate([y_true, labels]) if y_true.size else labels

    fpr, tpr, thresholds = roc_curve(y_true, y_probs)
    youden = tpr - fpr
    optimal_idx = np.argmax(youden)
    optimal_threshold = thresholds[optimal_idx]
    print(f"[ROC] Optimal threshold: {optimal_threshold:.4f}")
    return optimal_threshold

#####################################################################
# 3. Precision-Recall Curve ê¸°ë°˜ (F1 Score ìµœëŒ€í™”)
def find_threshold_pr(model, train_loader=None, val_loader=None, use_train=False, device="cuda"):
    """
    Precision-Recall Curveë¥¼ ê¸°ë°˜ìœ¼ë¡œ F1 Scoreê°€ ìµœëŒ€ê°€ ë˜ëŠ” ì„ê³„ê°’ì„ ì„ íƒí•©ë‹ˆë‹¤.
    """
    y_probs, y_true = np.array([]), np.array([])
    if use_train and train_loader is not None:
        probs, labels = collect_predictions(model, train_loader, device)
        y_probs = np.concatenate([y_probs, probs]) if y_probs.size else probs
        y_true = np.concatenate([y_true, labels]) if y_true.size else labels
    if val_loader is not None:
        probs, labels = collect_predictions(model, val_loader, device)
        y_probs = np.concatenate([y_probs, probs]) if y_probs.size else probs
        y_true = np.concatenate([y_true, labels]) if y_true.size else labels

    precisions, recalls, thresh = precision_recall_curve(y_true, y_probs)
    # ê³„ì‚° ì‹œ division by zero ë°©ì§€ë¥¼ ìœ„í•´ ì•„ì£¼ ì‘ì€ ê°’ì„ ì¶”ê°€
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
    # precision_recall_curveì˜ thresh ê¸¸ì´ëŠ” len(precisions)-1ì´ë¯€ë¡œ ì£¼ì˜
    optimal_idx = np.argmax(f1_scores)
    if optimal_idx >= len(thresh):
        optimal_threshold = thresh[-1]
    else:
        optimal_threshold = thresh[optimal_idx]
    print(f"[PR-F1] Optimal threshold: {optimal_threshold:.4f}")
    return optimal_threshold

#####################################################################
# 4. Grid Search ê¸°ë°˜ (F1 Score ìµœëŒ€í™”)
def find_threshold_grid_f1(model, train_loader=None, val_loader=None, use_train=False, device="cuda", num_candidates=100):
    """
    í›„ë³´ ì„ê³„ê°’ë“¤ì„ grid searchí•˜ì—¬ F1 Scoreê°€ ìµœëŒ€ê°€ ë˜ëŠ” ì„ê³„ê°’ì„ ì„ íƒí•©ë‹ˆë‹¤.
    """
    y_probs, y_true = np.array([]), np.array([])
    if use_train and train_loader is not None:
        probs, labels = collect_predictions(model, train_loader, device)
        y_probs = np.concatenate([y_probs, probs]) if y_probs.size else probs
        y_true = np.concatenate([y_true, labels]) if y_true.size else labels
    if val_loader is not None:
        probs, labels = collect_predictions(model, val_loader, device)
        y_probs = np.concatenate([y_probs, probs]) if y_probs.size else probs
        y_true = np.concatenate([y_true, labels]) if y_true.size else labels

    candidates = np.linspace(0, 1, num_candidates)
    best_threshold = 0.5
    best_f1 = 0.0
    for t in candidates:
        y_pred = (y_probs >= t).astype(int)
        current_f1 = f1_score(y_true, y_pred, zero_division=1)
        if current_f1 > best_f1:
            best_f1 = current_f1
            best_threshold = t
    print(f"[Grid-F1] Optimal threshold: {best_threshold:.4f} (F1: {best_f1:.4f})")
    return best_threshold

#####################################################################
# 5. ê³ ì • ì„ê³„ê°’
def find_threshold_fixed(model, train_loader=None, val_loader=None, use_train=False, device="cuda", fixed_value=0.5):
    """
    ê³ ì • ì„ê³„ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"[Fixed] Threshold: {fixed_value:.4f}")
    return fixed_value

def load_data(data_kind,tokenizer):
    article_to_title = {
        '1': '[ëª©ì ]', '2': '[ê¸°ë³¸ì›ì¹™]', '3': '[ê³µì •ê±°ë˜ ì¤€ìˆ˜ ë° ë™ë°˜ì„±ì¥ ì§€ì›]', '4': '[ìƒí’ˆì˜ ë‚©í’ˆ]', '5': '[ê²€ìˆ˜ê¸°ì¤€ ë° í’ˆì§ˆê²€ì‚¬]',
        '6': '[ë‚©í’ˆëŒ€ê¸ˆ ì§€ê¸‰ ë° ê°ì•¡ê¸ˆì§€]', '6-2': '[ê³µê¸‰ì›ê°€ ë³€ë™ì— ë”°ë¥¸ ë‚©í’ˆ ê°€ê²©ì˜ ì¡°ì •]', '7': '[ìƒí’ˆì˜ ë°˜í’ˆ]', '8': '[íŒë§¤ì¥ë ¤ê¸ˆ]',
        '9': '[íŒì´‰ì‚¬ì› íŒŒê²¬ ë“±]', '10': '[ì„œë¹„ìŠ¤ í’ˆì§ˆìœ ì§€]', '11': '[íŒì´‰í–‰ì‚¬ ì°¸ì—¬ ë“±]', '12': '[ë§¤ì¥ ìœ„ì¹˜ ë° ë©´ì  ë“±]',
        '12-2': '[ë§¤ì¥ì´ë™ ê¸°ì¤€ ë“±ì˜ ì‚¬ì „ í†µì§€]', '13': '[ê¸°íƒ€ ë¹„ìš©ì˜ ì‚¬ì „ í†µì§€]', '14': '[ê²½ì˜ì •ë³´ ì œê³µ ìš”êµ¬ ê¸ˆì§€]',
        '15': '[ë³´ë³µì¡°ì¹˜ì˜ ê¸ˆì§€]', '16': '[ê°ì¢… ë¶ˆì´ìµ ì œê³µ ê¸ˆì§€ ë“±]', '17': '[ì†í•´ë°°ìƒ]', '18': '[ì§€ì‹ì¬ì‚°ê¶Œ ë“±]',
        '19': '[ìƒí‘œê´€ë ¨íŠ¹ì•½]', '20': '[ì œì¡°ë¬¼ì±…ì„]', '21': '[ê¶Œë¦¬ã†ì˜ë¬´ì˜ ì–‘ë„ê¸ˆì§€]', '22': '[í†µì§€ì˜ë¬´]', '23': '[ë¹„ë°€ìœ ì§€]',
        '24': '[ê³„ì•½í•´ì§€]', '25': '[ìƒê³„]', '26': '[ê³„ì•½ì˜ ìœ íš¨ê¸°ê°„ ë° ê°±ì‹ ]', '26-2': '[ê³„ì•½ì˜ ê°±ì‹  ê¸°ì¤€ ë“±ì˜ ì‚¬ì „ í†µì§€]',
        '27': '[ë¶„ìŸí•´ê²° ë° ì¬íŒê´€í• ]', '28': '[ê³„ì•½ì˜ íš¨ë ¥]'
    }
    directory_path = f'./Data_Analysis/Data_ver2/toxic_data/'
    files_to_merge = [f for f in os.listdir(directory_path) if 'preprocessing' in f and f.endswith('.csv')]
    merged_df_toxic = pd.DataFrame()
    for file in files_to_merge:
        file_path = os.path.join(directory_path, file)
        df = pd.read_csv(file_path)
        df = df[['sentence', 'toxic_label', 'article_number']]
        merged_df_toxic = pd.concat([merged_df_toxic, df], ignore_index=True)
    merged_df_toxic["article_number"] = merged_df_toxic["article_number"].astype(str)
    merged_df_toxic["unfair_label"] = 0
    directory_path = './Data_Analysis/Data_ver2/unfair_data/'
    files_to_merge = [f for f in os.listdir(directory_path) if 'preprocessing' in f and f.endswith('.csv')]
    merged_df_unfair = pd.DataFrame()
    for file in files_to_merge:
        file_path = os.path.join(directory_path, file)
        df = pd.read_csv(file_path)
        df = df[['sentence', 'unfair_label', 'article_number']]
        merged_df_unfair = pd.concat([merged_df_unfair, df], ignore_index=True)
    merged_df_unfair["article_number"] = merged_df_unfair["article_number"].astype(str)
    merged_df_unfair["toxic_label"] = 0
    merged_df = pd.concat([merged_df_toxic, merged_df_unfair],ignore_index=True)
    merged_df = merged_df.drop_duplicates()
    print(f'Total merged_df: {len(merged_df)}')

    if data_kind == 'toxic':
        merged_df = merged_df.loc[merged_df['unfair_label']==0]
        merged_df = merged_df[['sentence', 'toxic_label', 'article_number']]
        merged_df.rename(columns={'toxic_label': 'label'}, inplace=True)

    elif data_kind == 'unfair':
        merged_df = merged_df[['sentence', 'unfair_label', 'article_number']]
        merged_df.rename(columns={'unfair_label': 'label'}, inplace=True)

    # articleë³„ ìƒ˜í”Œ ìˆ˜ 4ê°œ ë¯¸ë§Œì¸ ê²½ìš° ë³µì œ
    article_counts = merged_df["article_number"].value_counts()
    for article, count in article_counts.items():
        if count < 4:
            sample_to_duplicate = merged_df[merged_df["article_number"] == article]
            num_copies = 4 - count
            merged_df = pd.concat([merged_df] + [sample_to_duplicate] * num_copies, ignore_index=True)

    # sentence ìƒì„±: [ì¡° ì œëª©] + ì‹¤ì œ ë¬¸ì¥
    merged_df["sentence"] = merged_df.apply(
        lambda row: f"{article_to_title.get(row['article_number'])} {row['sentence']}", axis=1
    )

    # Train/Test ë¶„í•  (stratify: article_number, unfair_label)
    x_temp, _, y_temp, _ = train_test_split(
        merged_df["sentence"].tolist(),
        merged_df[["label", "article_number"]],
        test_size=0.1,
        random_state=42,
        stratify=merged_df[["article_number", "label"]],
        shuffle=True
    )
    y_temp_labels = y_temp["label"]

    # Train/Validation ë¶„í•  (8:2)
    X_train, X_val, y_train, y_val = train_test_split(
        x_temp,
        y_temp_labels,
        test_size=0.2,
        random_state=42,
        stratify=y_temp[["article_number", "label"]],
        shuffle=True
    )

    # ë°ì´í„° í† í°í™”: ê° ëª¨ë¸ë³„ tokenizer ì‚¬ìš© (Train/Val)
    X_train_ids, X_train_mask = tokenize_data(X_train, tokenizer)
    X_val_ids, X_val_mask = tokenize_data(X_val, tokenizer)
    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)
    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1)

    train_dataset = TensorDataset(X_train_ids, X_train_mask, y_train_tensor)
    val_dataset = TensorDataset(X_val_ids, X_val_mask, y_val_tensor)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

    return train_loader, val_loader

#####################################################################
# 6. ëª¨ë“  ì„ê³„ê°’ì„ ë°˜í™˜í•˜ëŠ” ì¢…í•© í•¨ìˆ˜
def find_all_thresholds(model,tokenizer ,data_kind, use_train=False, device="cuda"):
    train_loader,val_loader = load_data(data_kind,tokenizer)
    """
    ì—¬ëŸ¬ ì„ê³„ê°’ ì„¤ì • ë°©ë²•ìœ¼ë¡œ ê³„ì‚°ëœ ì„ê³„ê°’ë“¤ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    thresholds = {}
    thresholds['kmeans'] = find_threshold_kmeans(model, train_loader, val_loader, use_train, device)
    thresholds['roc'] = find_threshold_roc(model, train_loader, val_loader, use_train, device)
    thresholds['pr'] = find_threshold_pr(model, train_loader, val_loader, use_train, device)
    thresholds['grid_f1'] = find_threshold_grid_f1(model, train_loader, val_loader, use_train, device)
    thresholds['fixed'] = find_threshold_fixed(model, train_loader, val_loader, use_train, device)
    return thresholds

def tokenize_data(sentences, tokenizer, max_length=256):
    encoding = tokenizer(
        sentences, padding=True, truncation=True, max_length=max_length, return_tensors="pt"
    )
    return encoding["input_ids"], encoding["attention_mask"]