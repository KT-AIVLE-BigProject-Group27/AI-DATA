import os
import sys
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from transformers import (
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)
import evaluate
from datasets import Dataset
# ê²½ë¡œ ì„¤ì •: summarization_models ëª¨ë“ˆ(ëª¨ë¸ ìƒì„± í•¨ìˆ˜ í¬í•¨)
sys.path.append(os.path.abspath("./AI"))
from summarization_models import get_kobart_model

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("gogamza/kobart-summarization")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


##############################################
# (0) ë°ì´í„° ì „ì²˜ë¦¬ ë° í† í°í™” í•¨ìˆ˜
##############################################
def tokenize_function(example, tokenizer, max_input_length=512, max_target_length=128):
    # ì…ë ¥ í…ìŠ¤íŠ¸ í† í°í™”
    model_inputs = tokenizer(example["text"], max_length=max_input_length, truncation=True)
    # ìš”ì•½ë¬¸(ë¼ë²¨) í† í°í™” (target tokenizer ì‚¬ìš©)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(example["summary"], max_length=max_target_length, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


##############################################
# (1) í‰ê°€ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ (ROUGE ê¸°ë°˜)
##############################################
def compute_metrics_fn(eval_pred, tokenizer):
    """ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µì„ ë””ì½”ë”©í•œ í›„ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°"""
    rouge = evaluate.load("rouge")
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    result = {key: value * 100 for key, value in result.items()}
    return result

##############################################
# (2) ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ (ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ í…ìŠ¤íŠ¸ì™€ ìš”ì•½ í† í°í™”)
##############################################
def preprocess_function(examples):
    # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í°í™” (ìµœëŒ€ ê¸¸ì´ 2048)
    model_inputs = tokenizer(examples["text"],max_length=1024,truncation=True,padding="max_length")
    # ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ í† í°í™” (ìµœëŒ€ ê¸¸ì´ 512)
    labels = tokenizer(examples["summary"],max_length=256,truncation=True,padding="max_length")
    # ëª¨ë¸ì˜ íƒ€ê¹ƒ ë ˆì´ë¸”ë¡œ í† í°í™”ëœ ìš”ì•½ ì¶”ê°€
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


# ëª¨ë¸ ê²°ê³¼ ì €ì¥ ë° ë¹„êµë¥¼ ìœ„í•œ ê¸°ë³¸ ê²½ë¡œ
base_save_path = os.path.join("E:/Model/ver2", "summarization_comparison")
os.makedirs(base_save_path, exist_ok=True)

# ë°ì´í„° ë¡œë“œ (CSV íŒŒì¼, ì»¬ëŸ¼: "input", "summary")

directory_path = r'./Data_Analysis/Data_ver2/summary_data/'
csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]
dfs = []
for file in csv_files:
    file_path = os.path.join(directory_path, file)
    df = pd.read_csv(file_path)
    dfs.append(df)

merged_df = pd.concat(dfs, ignore_index=True)
merged_df['article'] = merged_df['input'].apply(lambda x: x.split('[')[0].strip())

import re

# ê³µë°± 2ê°œ ì´ìƒì„ 1ê°œë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def clean_whitespace(text):
    text = text.replace('â€œ','').replace('â€','').replace("'",'').replace('"','')
    return re.sub(r'\s{2,}', ' ', text.strip())

# 'input'ê³¼ 'summary' ì»¬ëŸ¼ì— ì ìš©
merged_df['input'] = merged_df['input'].apply(clean_whitespace)
merged_df['summary'] = merged_df['summary'].apply(clean_whitespace)


# âœ… 'input'ê³¼ 'summary'ì˜ í† í° ê¸¸ì´ ê³„ì‚°
merged_df["input_token_length"] = merged_df["input"].apply(lambda x: len(tokenizer.encode(x)))
merged_df["summary_token_length"] = merged_df["summary"].apply(lambda x: len(tokenizer.encode(x)))

# âœ… í† í° ê¸°ì¤€ ìµœëŒ€ ê¸¸ì´ ì¶œë ¥
max_input_token_length = merged_df["input_token_length"].max()
max_summary_token_length = merged_df["summary_token_length"].max()

print(f"ğŸ“Œ ìµœëŒ€ input í† í° ê¸¸ì´: {max_input_token_length}")
print(f"ğŸ“Œ ìµœëŒ€ summary í† í° ê¸¸ì´: {max_summary_token_length}")

# âœ… 1024 í† í° ì´ìƒì¸ ë°ì´í„° í™•ì¸
long_token_texts = merged_df[merged_df["input_token_length"] > 1024]
print(long_token_texts)


# datasetì€ "train" í‚¤ë¡œ ë¡œë“œë¨; Train/Test Split (ì˜ˆ: 90% train, 10% test)
X_train,X_val,y_train,y_val = train_test_split(merged_df['input'],merged_df['summary'],test_size=0.2, random_state=42,stratify=merged_df['article'] )
train_data = pd.DataFrame({'text': X_train, 'summary': y_train})
val_data = pd.DataFrame({'text': X_val, 'summary': y_val})
print(f'X_train: {len(X_train)}, X_val: {len(X_val)}')

# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸: (ëª¨ë¸ëª…, ìƒì„± í•¨ìˆ˜)
models_info = [
    ("kobart", get_kobart_model),
]

# í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
num_train_epochs = 1000
per_device_train_batch_size = 4
per_device_eval_batch_size = 4
learning_rate = 2e-5

# ê° ëª¨ë¸ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ ë° í‰ê°€
for model_label, get_model_fn in models_info:
    print("\n" + "=" * 50)
    print(f"ëª¨ë¸: {model_label} í•™ìŠµ ì‹œì‘")

    # 1. ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì • ë° ë””ë ‰í† ë¦¬ ìƒì„±
    cur_save_path = os.path.join(base_save_path, model_label)
    os.makedirs(cur_save_path, exist_ok=True)

    # 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = get_model_fn()
    model.to(device)

    # 3. ë°ì´í„°ì…‹ ë³€í™˜ (pandas â†’ Hugging Face Dataset)
    train_dataset = Dataset.from_pandas(train_data)
    val_dataset = Dataset.from_pandas(val_data)

    # 5. ë°ì´í„° í† í°í™” (batched=True ì ìš©, ê¸°ì¡´ ì»¬ëŸ¼ ì œê±°)
    tokenized_train = train_dataset.map(
        preprocess_function, batched=True, remove_columns=train_dataset.column_names
    )
    tokenized_val = val_dataset.map(
        preprocess_function, batched=True, remove_columns=val_dataset.column_names
    )

    def compute_metrics(eval_pred):
        return compute_metrics_fn(eval_pred, tokenizer)

    # 6. TrainingArguments ì„¤ì • (í•™ìŠµ ê´€ë ¨ ì¸ìë“¤ ì§€ì •)
    training_args = Seq2SeqTrainingArguments(
        output_dir=os.path.join(cur_save_path, "results"),
        evaluation_strategy="epoch",
        save_strategy="epoch",  # ë§¤ epochë§ˆë‹¤ ì €ì¥
        save_total_limit=3,
        load_best_model_at_end=True,  # ğŸ”¹ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì €ì¥ (restore_best_weights í¬í•¨)
        metric_for_best_model="eval_loss",  # ğŸ”¹ í‰ê°€ ê¸°ì¤€ (eval_lossê°€ ê°€ì¥ ë‚®ì„ ë•Œ ì €ì¥)
        greater_is_better=False,  # ğŸ”¹ LossëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        learning_rate=learning_rate,
        per_device_train_batch_size=per_device_train_batch_size,
        per_device_eval_batch_size=per_device_eval_batch_size,
        weight_decay=0.01,
        num_train_epochs=num_train_epochs,
        predict_with_generate=True,
        fp16=True if device.type == "cuda" else False,
        logging_dir=os.path.join(cur_save_path, "logs"),
        remove_unused_columns=False,  # "No columns" ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ì‚¬ìš©
    )

    # 8. Seq2SeqTrainer ì„¤ì • (ëª¨ë¸, ë°ì´í„°ì…‹, í† í¬ë‚˜ì´ì €, í‰ê°€ í•¨ìˆ˜ ë“± ë“±ë¡)
    from transformers import EarlyStoppingCallback

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]  # ğŸ”¹ Early Stopping ì ìš©
    )

    # 9. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
    trainer.train()

    # ğŸ”¹ ì¶”ê°€ 1: Loss ë°ì´í„° ì €ì¥ (CSV ì €ì¥)
    loss_history = pd.DataFrame(trainer.state.log_history)
    loss_history.to_csv(os.path.join(cur_save_path, "loss_history.csv"), index=False)

    # ğŸ”¹ ì¶”ê°€ 2: Loss ê·¸ë˜í”„ ì €ì¥
    plt.figure(figsize=(6, 4))
    train_losses = [entry["loss"] for entry in trainer.state.log_history if "loss" in entry]
    eval_losses = [entry["eval_loss"] for entry in trainer.state.log_history if "eval_loss" in entry]

    plt.plot(train_losses, label="Train Loss", marker="o")
    plt.plot(eval_losses, label="Validation Loss", marker="o")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(f"{model_label} Loss Curve")
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(cur_save_path, "loss_curve.png"))
    plt.close()

    # 10. í•™ìŠµ í›„ í‰ê°€ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥
    eval_results = trainer.evaluate()
    print(f"[{model_label}] í‰ê°€ ê²°ê³¼:")
    print(eval_results)

    # 11. í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
    model_save_path = os.path.join(cur_save_path, f"{model_label}_summarization")
    os.makedirs(model_save_path, exist_ok=True)
    model.save_pretrained(model_save_path)
    tokenizer.save_pretrained(model_save_path)
    print(f"âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ: {model_save_path}")


# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ëª¨ë¸ëª…ê³¼ ìƒì„± í•¨ìˆ˜)
models_info = [
    ("kobart", get_kobart_model),
]
results = []  # ê° ëª¨ë¸ í‰ê°€ ì§€í‘œ ì €ì¥
base_save_path = "E:/Model/ver2/summarization_comparison/"  # ì €ì¥ëœ ëª¨ë¸ë“¤ì´ ìœ„ì¹˜í•œ ê¸°ë³¸ ê²½ë¡œ (ì˜ˆì‹œ)

for model_label, get_model_fn in models_info:
    print("\n" + "=" * 50)
    print(f"ëª¨ë¸: {model_label} í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # ê° ëª¨ë¸ ì „ìš© save ê²½ë¡œ
    cur_save_path = os.path.join(base_save_path, model_label)
    model_save_path = os.path.join(cur_save_path, f"{model_label}_summarization")

    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ìƒì„± (ëª¨ë¸ì€ raw logits/ìƒì„± ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë„ë¡ êµ¬í˜„ë˜ì–´ ìˆì–´ì•¼ í•¨)
    model, tokenizer = get_model_fn()

    # ì €ì¥ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (í´ë˜ìŠ¤ ë©”ì„œë“œ ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ)
    model = model.__class__.from_pretrained(model_save_path)
    tokenizer = type(tokenizer).from_pretrained(model_save_path)
    model.to(device)

    # 3. ë°ì´í„°ì…‹ ë³€í™˜ (pandas â†’ Hugging Face Dataset)
    train_dataset = Dataset.from_pandas(train_data)
    val_dataset = Dataset.from_pandas(val_data)

    # 5. ë°ì´í„° í† í°í™” (batched=True ì ìš©, ê¸°ì¡´ ì»¬ëŸ¼ ì œê±°)
    tokenized_train = train_dataset.map(
        preprocess_function, batched=True, remove_columns=train_dataset.column_names
    )
    tokenized_val = val_dataset.map(
        preprocess_function, batched=True, remove_columns=val_dataset.column_names
    )
    # Trainer í‰ê°€ë¥¼ ìœ„í•œ TrainingArguments (í‰ê°€ë§Œ ì§„í–‰í•˜ë¯€ë¡œ ê°„ë‹¨í•œ ì„¤ì •)
    training_args = Seq2SeqTrainingArguments(
        output_dir="./dummy_output",
        per_device_eval_batch_size=4,
        predict_with_generate=True,
        evaluation_strategy="no",  # í‰ê°€ë§Œ ìˆ˜í–‰
        remove_unused_columns = False
    )

    # Trainer ì´ˆê¸°í™” (compute_metricsì€ ìƒëµí•˜ê±°ë‚˜ ì¶”ê°€ ê°€ëŠ¥)
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        eval_dataset=tokenized_val,
        tokenizer=tokenizer,
    )

    # í‰ê°€ ì‹¤í–‰
    eval_results = trainer.evaluate()
    print(f"[{model_label}] í‰ê°€ ê²°ê³¼:")
    print(eval_results)

    # ê²°ê³¼ ì €ì¥: ì—¬ê¸°ì„œëŠ” eval_lossì™€ (ìˆë‹¤ë©´) ROUGE-L ì ìˆ˜ë¥¼ ì €ì¥
    result_entry = {
        "Model": model_label,
        "Eval_Loss": eval_results.get("eval_loss", np.nan),
        "RougeL": eval_results.get("eval_rougeL", np.nan)
    }
    results.append(result_entry)


    # ëª¨ë¸ í‰ê°€ ë° ì˜ˆì¸¡ ìˆ˜í–‰
    predictions = trainer.predict(tokenized_val)
    decoded_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode([label for label in predictions.label_ids], skip_special_tokens=True)
    original_inputs = val_data["text"].tolist()
    results_df = pd.DataFrame({
        "Input": original_inputs,
        "Predicted_Summary": decoded_preds,
        "Ground_Truth_Summary": decoded_labels
    })
    results_csv_path = f"{cur_save_path}/{model_label}_summarization_results.csv"
    results_df.to_csv(results_csv_path, index=False, encoding="utf-8-sig")

    print(f"âœ… ëª¨ë¸ ì…ë ¥/ì¶œë ¥ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_csv_path}")

# ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” (ëª¨ë¸ë³„ í‰ê°€ ì§€í‘œ)
results_df = pd.DataFrame(results)
print("\nì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
print(results_df)

# ê° í‰ê°€ ì§€í‘œì— ëŒ€í•´ bar chart ìƒì„±
metrics = ["Eval_Loss", "RougeL"]
num_metrics = len(metrics)
fig, axs = plt.subplots(1, num_metrics, figsize=(5 * num_metrics, 5))
if num_metrics == 1:
    axs = [axs]
for i, metric in enumerate(metrics):
    axs[i].bar(results_df["Model"], results_df[metric], color="skyblue")
    axs[i].set_title(metric)
    axs[i].set_ylim([0, results_df[metric].max() * 1.2])
    for j, value in enumerate(results_df[metric]):
        axs[i].text(j, value + 0.02 * results_df[metric].max(), f"{value:.2f}", ha="center", va="bottom", fontsize=10)
plt.suptitle("ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ", fontsize=16)
comparison_plot_file = os.path.join(base_save_path, "model_comparison.png")
plt.tight_layout()
plt.savefig(comparison_plot_file)
plt.show()
