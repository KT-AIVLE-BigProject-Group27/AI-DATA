#pip install torch transformers datasets scikit-learn
import torch
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset, DatasetDict


name = 'toxic_(klue_bert_base)_ver1_1ì°¨'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
data = pd.read_csv('./Data_Analysis/Data/toxic_sentence_merged.csv')

# âœ… í›ˆë ¨ ë°ì´í„° & í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (80:20)
train_texts, test_texts, train_labels, test_labels = train_test_split(
    data["sentence"].tolist(), data["label"].tolist(), test_size=0.2, random_state=42, stratify=data["label"]
)

# âœ… BERT í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# âœ… í…ìŠ¤íŠ¸ í† í°í™” í•¨ìˆ˜ (Hugging Face Dataset í™œìš©)
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# âœ… DatasetDict ìƒì„± (Hugging Face Datasets ì‚¬ìš©)
train_dataset = Dataset.from_dict({"text": train_texts, "label": train_labels})
test_dataset = Dataset.from_dict({"text": test_texts, "label": test_labels})

# âœ… í† í°í™” ì ìš©
train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)
dataset = DatasetDict({"train": train_dataset, "test": test_dataset})

def convert_labels_to_float(dataset):
    return dataset.map(lambda x: {"labels": float(x["label"])})

# í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë³€í™˜
dataset["train"] = convert_labels_to_float(dataset["train"])
dataset["test"] = convert_labels_to_float(dataset["test"])
##################################################################################################
##################################################################################################

# ì‚¬ì „ í›ˆë ¨ëœ BERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ì´ì§„ ë¶„ë¥˜ìš©)
output_dir = f"./Data_Analysis/Model/{name}/ckp/"
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=1).to(device)

# í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì • (ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ ì§€, ReduceLROnPlateau ì¶”ê°€, Early Stopping ì¶”ê°€)
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",  # âœ… í‰ê°€ ì£¼ê¸°ë¥¼ epoch ë‹¨ìœ„ë¡œ ì„¤ì •
    save_strategy="epoch",  # âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ ì§€ (Early Stoppingê³¼ ì¼ì¹˜í•´ì•¼ í•¨)
    num_train_epochs=1000,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    load_best_model_at_end=True,  # âœ… Early Stoppingì´ ìµœì  ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆë„ë¡ True ì„¤ì •
    metric_for_best_model="eval_loss",
    lr_scheduler_type="reduce_lr_on_plateau",  # âœ… ReduceLROnPlateau ì ìš©
    save_total_limit=3,  # âœ… ì €ì¥í•  ì²´í¬í¬ì¸íŠ¸ ê°œìˆ˜ ì œí•œ (í•„ìš”ì‹œ ë³€ê²½ ê°€ëŠ¥)
)

from transformers import Trainer, EarlyStoppingCallback

# âœ… Early Stopping ì½œë°± ì¶”ê°€ (patience=10)
early_stopping = EarlyStoppingCallback(early_stopping_patience=10)

# Trainer ì„¤ì • (Hugging Face Trainer API ì‚¬ìš©)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    callbacks=[early_stopping],  # âœ… Early Stopping ì¶”ê°€
)

# ëª¨ë¸ í•™ìŠµ ì‹œì‘
train_result = trainer.train()


def plot_loss_curve(train_loss, val_loss, save_path):
    plt.figure(figsize=(10, 6))
    plt.plot(train_loss, label="Train Loss", marker="o")
    plt.plot(val_loss, label="Validation Loss", marker="o")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training and Validation Loss")
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path)
    print(f"âœ… Loss ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")
    plt.close()

# âœ… í•™ìŠµ ì†ì‹¤ ë° ê²€ì¦ ì†ì‹¤ ì €ì¥
# âœ… í•™ìŠµ ì†ì‹¤ ë° ê²€ì¦ ì†ì‹¤ ì €ì¥
train_losses = []
val_losses = []

for log in trainer.state.log_history:
    if "loss" in log:
        train_losses.append(log["loss"])
    if "eval_loss" in log:
        val_losses.append(log["eval_loss"])

# âœ… ì†ì‹¤ ê·¸ë˜í”„ ì €ì¥
save_path = f"./Data_Analysis/Model/{name}/loss_curve.png"
plot_loss_curve(train_losses, val_losses, save_path)

###########################################################################
###########################################################################
###########################################################################

import os
save_dir = f"./Data_Analysis/Model/{name}/"
os.makedirs(save_dir, exist_ok=True)
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

###########################################################################
###########################################################################
###########################################################################

# âœ… ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
def load_trained_model(model_path):
    return BertForSequenceClassification.from_pretrained(model_path).to(device)

# âœ… ëª¨ë¸ ë¡œë“œ ì‹¤í–‰
loaded_model = load_trained_model(save_dir)
loaded_tokenizer = BertTokenizer.from_pretrained(save_dir)  # âœ… í† í¬ë‚˜ì´ì €ë„ ê°™ì€ ê²½ë¡œì—ì„œ ë¡œë“œ


##################################################################################################
##################################################################################################
##################################################################################################
def predict(text):
    loaded_model.eval()  # í‰ê°€ ëª¨ë“œ ì„¤ì •
    inputs = loaded_tokenizer(text, return_tensors="pt", padding="max_length", truncation=True).to(device)
    with torch.no_grad():
        logits = loaded_model(**inputs).logits  # ëª¨ë¸ ì˜ˆì¸¡ê°’
        prob = torch.sigmoid(logits).item()  # í™•ë¥  ë³€í™˜
    return prob  # ë…ì†Œ ì¡°í•­ì¼ í™•ë¥  (0~1 ì‚¬ì´ ê°’)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°
test_data = [
    ["ì„ì€ ê°‘ì˜ ìš”ì²­ì´ ìˆì„ ê²½ìš°, ì •í•´ì§„ ê³„ì•½ ê¸°ê°„ê³¼ ê´€ê³„ì—†ì´ ì¶”ê°€ì ì¸ ë‚©í’ˆì„ ì§„í–‰í•´ì•¼ í•œë‹¤.", 1],
    ["ê°‘ì€ ì„ì˜ ì¬ê³  ìƒí™©ê³¼ ê´€ê³„ì—†ì´ ì£¼ë¬¸ëŸ‰ì„ ììœ ë¡­ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìœ¼ë©°, ì„ì€ ì´ì— ë¬´ì¡°ê±´ ì‘í•´ì•¼ í•œë‹¤.", 1],
    ["ì„ì€ ê°‘ì˜ íŒë§¤ ì „ëµì— ë”°ë¼ ì›ê°€ ì´í•˜ì˜ ê°€ê²©ìœ¼ë¡œ ë‚©í’ˆí•´ì•¼ í•˜ë©°, ì´ì— ëŒ€í•œ ì†ì‹¤ ë³´ì „ì„ ìš”êµ¬í•  ìˆ˜ ì—†ë‹¤.", 1],
    ["ë³¸ ê³„ì•½ ì²´ê²° ì´í›„ì—ë„ ê°‘ì€ ì„ì˜ ìœ í†µë§ì„ ì§ì ‘ í†µì œí•  ìˆ˜ ìˆìœ¼ë©°, ì„ì€ ì´ë¥¼ ê±°ë¶€í•  ìˆ˜ ì—†ë‹¤.", 1],
    ["ì„ì€ ê°‘ì˜ ê²½ì˜ ì „ëµì— ë”°ë¼ ê°€ê²© ë° íŒë§¤ ì •ì±…ì„ ì¡°ì •í•´ì•¼ í•˜ë©°, ì´ì— ëŒ€í•œ í˜‘ì˜ ê¶Œí•œì´ ì—†ë‹¤.", 1],
    ["ê°‘ì€ ì„ì˜ ë‚©í’ˆ ê¸°í•œì„ ì‚¬ì „ í˜‘ì˜ ì—†ì´ ì¡°ì •í•  ìˆ˜ ìˆìœ¼ë©°, ì„ì€ ì´ì— ì¦‰ì‹œ ì‘í•´ì•¼ í•œë‹¤.", 1],
    ["ì„ì€ ê°‘ì˜ íŒë§¤ ì´‰ì§„ì„ ìœ„í•´ ì¶”ê°€ì ì¸ ì œí’ˆì„ ë¬´ìƒìœ¼ë¡œ ì œê³µí•´ì•¼ í•˜ë©°, ì´ì— ëŒ€í•œ ëŒ€ê°€ë¥¼ ìš”êµ¬í•  ìˆ˜ ì—†ë‹¤.", 1],
    ["ë³¸ ê³„ì•½ì˜ ì¢…ë£Œ ì—¬ë¶€ëŠ” ê°‘ì´ ë‹¨ë…ìœ¼ë¡œ ê²°ì •í•˜ë©°, ì„ì€ ì´ì— ëŒ€í•´ ì–´ë– í•œ ê¶Œë¦¬ë„ ì£¼ì¥í•  ìˆ˜ ì—†ë‹¤.", 1],
    ["ê°‘ì€ ì„ì˜ ìƒì‚° ê³¼ì •ì— ê°œì…í•  ê¶Œë¦¬ë¥¼ ê°€ì§€ë©°, ì„ì€ ì´ì— ëŒ€í•´ ê±°ë¶€í•  ìˆ˜ ì—†ë‹¤.", 1],
    ["ì„ì€ ê³„ì•½ì´ ì¢…ë£Œëœ í›„ì—ë„ ì¼ì • ê¸°ê°„ ë™ì•ˆ ê°‘ì´ ìš”ì²­í•˜ëŠ” ì¡°ê±´ì„ ìœ ì§€í•˜ì—¬ ì œí’ˆì„ ê³µê¸‰í•´ì•¼ í•œë‹¤.", 1],
    ["ê³„ì•½ ë‹¹ì‚¬ìëŠ” ê³„ì•½ì˜ ì´í–‰ì„ ìœ„í•´ ìƒí˜¸ í˜‘ë ¥í•˜ë©°, ë¬¸ì œ ë°œìƒ ì‹œ í˜‘ì˜ë¥¼ í†µí•´ í•´ê²°í•´ì•¼ í•œë‹¤.", 0],
    ["ì„ì€ ê³„ì•½ëœ ì¼ì •ì— ë”°ë¼ ì œí’ˆì„ ë‚©í’ˆí•˜ë©°, ì¼ì • ë³€ê²½ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ì „ì— í˜‘ì˜í•œë‹¤.", 0],
    ["ë³¸ ê³„ì•½ì˜ ì¡°í•­ì€ ì–‘ì¸¡ì˜ ë™ì˜ ì—†ì´ ì¼ë°©ì ìœ¼ë¡œ ë³€ê²½ë  ìˆ˜ ì—†ë‹¤.", 0],
    ["ê³„ì•½ í•´ì§€ ì‹œ, ë‹¹ì‚¬ìëŠ” í•©ì˜ëœ ì ˆì°¨ì— ë”°ë¼ ì„œë©´ìœ¼ë¡œ í†µë³´í•´ì•¼ í•œë‹¤.", 0],
    ["ê°‘ì€ ì„ì˜ ì •ë‹¹í•œ ì‚¬ìœ  ì—†ì´ ê³„ì•½ ì¡°ê±´ì„ ì¼ë°©ì ìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ ì—†ë‹¤.", 0],
    ["ì„ì€ ê³„ì•½ ì´í–‰ ì¤‘ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ ê°‘ì—ê²Œ ì¦‰ì‹œ ë³´ê³ í•˜ê³  í˜‘ì˜í•´ì•¼ í•œë‹¤.", 0],
    ["ë³¸ ê³„ì•½ì€ ê³„ì•½ì„œì— ëª…ì‹œëœ ê¸°í•œ ë™ì•ˆ ì ìš©ë˜ë©°, ì—°ì¥ì€ ì–‘ì¸¡ í˜‘ì˜ë¥¼ í†µí•´ ì§„í–‰ëœë‹¤.", 0],
    ["ê³„ì•½ ë‹¹ì‚¬ìëŠ” ìƒí˜¸ ì¡´ì¤‘ì„ ë°”íƒ•ìœ¼ë¡œ ê³„ì•½ì„ ì´í–‰í•˜ë©°, í•„ìš” ì‹œ í˜‘ì˜ë¥¼ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.", 0],
    ["ê³„ì•½ ì¢…ë£Œ í›„ì—ë„ ë‹¹ì‚¬ìëŠ” ì¼ì • ê¸°ê°„ ë™ì•ˆ ê¸°ë°€ ìœ ì§€ ì˜ë¬´ë¥¼ ì¤€ìˆ˜í•´ì•¼ í•œë‹¤.", 0],
    ["ë³¸ ê³„ì•½ì—ì„œ ëª…ì‹œë˜ì§€ ì•Šì€ ì‚¬í•­ì€ ê´€ë ¨ ë²•ë ¹ ë° ì¼ë°˜ì ì¸ ìƒê±°ë˜ ê´€í–‰ì„ ë”°ë¥¸ë‹¤.", 0],
]


from torch.utils.data import DataLoader, TensorDataset
import torch
train_encodings = tokenizer(
    dataset["train"]["text"], padding="max_length", truncation=True, max_length=256, return_tensors="pt"
)
test_encodings = tokenizer(
    dataset["test"]["text"], padding="max_length", truncation=True, max_length=256, return_tensors="pt"
)
# âœ… ë ˆì´ë¸”ì„ Tensorë¡œ ë³€í™˜
train_labels = torch.tensor(dataset["train"]["label"], dtype=torch.float32).unsqueeze(1)  # [batch, 1] í˜•íƒœ
test_labels = torch.tensor(dataset["test"]["label"], dtype=torch.float32).unsqueeze(1)

# âœ… TensorDataset ìƒì„±
train_tensor_dataset = TensorDataset(train_encodings["input_ids"], train_encodings["attention_mask"], train_labels)
test_tensor_dataset = TensorDataset(test_encodings["input_ids"], test_encodings["attention_mask"], test_labels)

# âœ… DataLoader ìƒì„±
batch_size = 16  # ë°°ì¹˜ í¬ê¸° ì„¤ì • (í•„ìš”í•˜ë©´ ë³€ê²½ ê°€ëŠ¥)
train_loader = DataLoader(train_tensor_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(test_tensor_dataset, batch_size=batch_size, shuffle=False)

"""
import os, sys
sys.path.append(os.path.abspath("./AI"))
import threshold_settings as ts
threshold= ts.find_threshold_2(loaded_model, train_loader=train_loader, val_loader=val_loader, use_train=False, device=device)
ìµœì  ì„ê³„ê°’: 0.6213
"""
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
y_pred = []
y_true = []
threshold = 0.6213


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥
print("\nğŸ”¹ **í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ê²°ê³¼** ğŸ”¹\n")
for text, label in test_data:
    probability = predict(text)
    print(f"ë¬¸ì¥: {text}")
    print(f"ì˜ˆì¸¡ ë…ì†Œ ì¡°í•­ í™•ë¥ : {probability*100:.2f}%")
    print("-" * 80)
    y_pred.append(1 if probability >= threshold else 0)
    y_true.append(label)

# âœ… ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
roc_auc = roc_auc_score(y_true, y_pred)

# âœ… ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ğŸ“Š")
print(f"ğŸ“Œ ìµœì  ì„ê³„ê°’: {threshold:.4f}")
print(f"âœ… Accuracy: {accuracy:.4f}")
print(f"âœ… Precision: {precision:.4f}")
print(f"âœ… Recall: {recall:.4f}")
print(f"âœ… F1-score: {f1:.4f}")
print(f"âœ… ROC-AUC: {roc_auc:.4f}")