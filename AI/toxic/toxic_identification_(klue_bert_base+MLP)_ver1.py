import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertTokenizer, BertModel
import pandas as pd
from sklearn.model_selection import train_test_split
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
name = 'toxic_(klue_bert_base_MLP)_ver1_1ì°¨'
model_name = "klue/bert-base"
tokenizer = BertTokenizer.from_pretrained(model_name)


save_path = f"./Data_Analysis/Model/{name}/"
os.makedirs(save_path, exist_ok=True)
model_file = os.path.join(save_path, "klue_bert_mlp.pth")


class BertMLPClassifier(nn.Module):
    def __init__(self, bert_model_name="klue/bert-base", hidden_size=256):
        super(BertMLPClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.fc1 = nn.Linear(self.bert.config.hidden_size, hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(hidden_size, 1)  # ë¶ˆê³µì •(1) í™•ë¥ ì„ ì¶œë ¥
        self.sigmoid = nn.Sigmoid()  # í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] ë²¡í„° ì‚¬ìš©
        x = self.fc1(cls_output)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return self.sigmoid(x)  # 0~1 í™•ë¥ ê°’ ë°˜í™˜


data = pd.read_csv('./Data_Analysis/Data/toxic_sentence_merged.csv')

print(f'data_shape: {data.shape}')

X_train, X_val, y_train, y_val = train_test_split(data["sentence"].tolist(), data["label"].tolist(), test_size=0.2, random_state=42, stratify=data["label"],shuffle=True)


def tokenize_data(sentences, tokenizer, max_length=256):
    encoding = tokenizer(
        sentences, padding=True, truncation=True, max_length=max_length, return_tensors="pt"
    )
    return encoding["input_ids"], encoding["attention_mask"]


X_train_ids, X_train_mask = tokenize_data(X_train, tokenizer)
X_val_ids, X_val_mask = tokenize_data(X_val, tokenizer)

y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # [batch, 1] í˜•íƒœ
y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)


# âœ… ëª¨ë¸ ì´ˆê¸°í™”

model = BertMLPClassifier().to(device)
criterion = nn.BCELoss()  # Binary Cross Entropy Loss ì‚¬ìš©
optimizer = optim.Adam(model.parameters(), lr=0.00002)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)

# âœ… ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (Early Stopping ì ìš©, ì €ì¥ X)
from torch.utils.data import DataLoader, TensorDataset

# âœ… ë°°ì¹˜ í¬ê¸° ì„¤ì •
batch_size = 16

# âœ… ë°ì´í„°ì…‹ & ë°ì´í„°ë¡œë” ì„¤ì •
train_dataset = TensorDataset(X_train_ids, X_train_mask, y_train_tensor)
val_dataset = TensorDataset(X_val_ids, X_val_mask, y_val_tensor)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

#######################################################################################################################
# train
#######################################################################################################################
import matplotlib.pyplot as plt
# âœ… ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ ìˆ˜ì •
# âœ… ì†ì‹¤ ê·¸ë˜í”„ ì €ì¥ í•¨ìˆ˜
def plot_loss_curve(train_losses, val_losses, save_path):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label="Train Loss", marker="o")
    plt.plot(val_losses, label="Validation Loss", marker="o")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training and Validation Loss")
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path)  # ì´ë¯¸ì§€ë¡œ ì €ì¥
    print(f"âœ… Loss ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")
    plt.close()

# âœ… ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ ìˆ˜ì • (ì†ì‹¤ ê·¸ë˜í”„ ì¶”ê°€)
def train_model(model, train_loader, val_loader, epochs=10, patience=3):
    best_loss = float('inf')
    patience_counter = 0
    train_loss_list = []
    val_loss_list = []

    # âœ… ReduceLROnPlateau ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€ (patience=2, factor=0.5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        # âœ… ë°°ì¹˜ í•™ìŠµ ì ìš©
        for X_batch, mask_batch, y_batch in train_loader:
            X_batch, mask_batch, y_batch = X_batch.to(device), mask_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            outputs = model(X_batch, mask_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        # âœ… ê²€ì¦ ë‹¨ê³„
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, mask_batch, y_batch in val_loader:
                X_batch, mask_batch, y_batch = X_batch.to(device), mask_batch.to(device), y_batch.to(device)
                val_outputs = model(X_batch, mask_batch)
                val_loss += criterion(val_outputs, y_batch).item()

        val_loss /= len(val_loader)

        # âœ… ì†ì‹¤ ì €ì¥
        train_loss_list.append(total_loss)
        val_loss_list.append(val_loss)

        # âœ… ReduceLROnPlateau ì ìš©
        scheduler.step(val_loss)

        print(f"Epoch [{epoch+1}/{epochs}] - Train Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}")

        # âœ… Early Stopping ì²´í¬
        if val_loss < best_loss:
            best_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch + 1}")
            break

    # âœ… ì†ì‹¤ ê·¸ë˜í”„ ì €ì¥
    loss_plot_path = os.path.join(save_path, "loss_curve.png")
    plot_loss_curve(train_loss_list, val_loss_list, loss_plot_path)

# âœ… ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ (ì†ì‹¤ ê·¸ë˜í”„ ì¶”ê°€ë¨)
train_model(model, train_loader, val_loader, epochs=1000, patience=10)

torch.save(model.state_dict(), model_file)
print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_file}")
#######################################################################################################################
#######################################################################################################################

def predict_toxic_clause(c_model, sentence, threshold=0.5):
    c_model.eval()
    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=256, return_tensors="pt").to(device)
    with torch.no_grad():
        output = c_model(inputs["input_ids"], inputs["attention_mask"])
        unfair_prob = output.item()
    return {
        "sentence": sentence,
        "toxic_probability": round(unfair_prob * 100, 2),  # 1(ë¶ˆê³µì •) í™•ë¥ 
        "predicted_label": "ë…ì†Œ" if unfair_prob >= threshold else "ë¹„ë…ì†Œ"
    }

test_data = [
    ("ì„ì€ ê°‘ì˜ ìš”ì²­ì´ ìˆì„ ê²½ìš°, ì •í•´ì§„ ê³„ì•½ ê¸°ê°„ê³¼ ê´€ê³„ì—†ì´ ì¶”ê°€ì ì¸ ë‚©í’ˆì„ ì§„í–‰í•´ì•¼ í•œë‹¤.", 1),
    ("ê°‘ì€ ì„ì˜ ì¬ê³  ìƒí™©ê³¼ ê´€ê³„ì—†ì´ ì£¼ë¬¸ëŸ‰ì„ ììœ ë¡­ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìœ¼ë©°, ì„ì€ ì´ì— ë¬´ì¡°ê±´ ì‘í•´ì•¼ í•œë‹¤.", 1),
    ("ì„ì€ ê°‘ì˜ íŒë§¤ ì „ëµì— ë”°ë¼ ì›ê°€ ì´í•˜ì˜ ê°€ê²©ìœ¼ë¡œ ë‚©í’ˆí•´ì•¼ í•˜ë©°, ì´ì— ëŒ€í•œ ì†ì‹¤ ë³´ì „ì„ ìš”êµ¬í•  ìˆ˜ ì—†ë‹¤.", 1),
    ("ë³¸ ê³„ì•½ ì²´ê²° ì´í›„ì—ë„ ê°‘ì€ ì„ì˜ ìœ í†µë§ì„ ì§ì ‘ í†µì œí•  ìˆ˜ ìˆìœ¼ë©°, ì„ì€ ì´ë¥¼ ê±°ë¶€í•  ìˆ˜ ì—†ë‹¤.", 1),
    ("ì„ì€ ê°‘ì˜ ê²½ì˜ ì „ëµì— ë”°ë¼ ê°€ê²© ë° íŒë§¤ ì •ì±…ì„ ì¡°ì •í•´ì•¼ í•˜ë©°, ì´ì— ëŒ€í•œ í˜‘ì˜ ê¶Œí•œì´ ì—†ë‹¤.", 1),
    ("ê°‘ì€ ì„ì˜ ë‚©í’ˆ ê¸°í•œì„ ì‚¬ì „ í˜‘ì˜ ì—†ì´ ì¡°ì •í•  ìˆ˜ ìˆìœ¼ë©°, ì„ì€ ì´ì— ì¦‰ì‹œ ì‘í•´ì•¼ í•œë‹¤.", 1),
    ("ì„ì€ ê°‘ì˜ íŒë§¤ ì´‰ì§„ì„ ìœ„í•´ ì¶”ê°€ì ì¸ ì œí’ˆì„ ë¬´ìƒìœ¼ë¡œ ì œê³µí•´ì•¼ í•˜ë©°, ì´ì— ëŒ€í•œ ëŒ€ê°€ë¥¼ ìš”êµ¬í•  ìˆ˜ ì—†ë‹¤.", 1),
    ("ë³¸ ê³„ì•½ì˜ ì¢…ë£Œ ì—¬ë¶€ëŠ” ê°‘ì´ ë‹¨ë…ìœ¼ë¡œ ê²°ì •í•˜ë©°, ì„ì€ ì´ì— ëŒ€í•´ ì–´ë– í•œ ê¶Œë¦¬ë„ ì£¼ì¥í•  ìˆ˜ ì—†ë‹¤.", 1),
    ("ê°‘ì€ ì„ì˜ ìƒì‚° ê³¼ì •ì— ê°œì…í•  ê¶Œë¦¬ë¥¼ ê°€ì§€ë©°, ì„ì€ ì´ì— ëŒ€í•´ ê±°ë¶€í•  ìˆ˜ ì—†ë‹¤.", 1),
    ("ì„ì€ ê³„ì•½ì´ ì¢…ë£Œëœ í›„ì—ë„ ì¼ì • ê¸°ê°„ ë™ì•ˆ ê°‘ì´ ìš”ì²­í•˜ëŠ” ì¡°ê±´ì„ ìœ ì§€í•˜ì—¬ ì œí’ˆì„ ê³µê¸‰í•´ì•¼ í•œë‹¤.", 1),
    ("ê³„ì•½ ë‹¹ì‚¬ìëŠ” ê³„ì•½ì˜ ì´í–‰ì„ ìœ„í•´ ìƒí˜¸ í˜‘ë ¥í•˜ë©°, ë¬¸ì œ ë°œìƒ ì‹œ í˜‘ì˜ë¥¼ í†µí•´ í•´ê²°í•´ì•¼ í•œë‹¤.", 0),
    ("ì„ì€ ê³„ì•½ëœ ì¼ì •ì— ë”°ë¼ ì œí’ˆì„ ë‚©í’ˆí•˜ë©°, ì¼ì • ë³€ê²½ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ì „ì— í˜‘ì˜í•œë‹¤.", 0),
    ("ë³¸ ê³„ì•½ì˜ ì¡°í•­ì€ ì–‘ì¸¡ì˜ ë™ì˜ ì—†ì´ ì¼ë°©ì ìœ¼ë¡œ ë³€ê²½ë  ìˆ˜ ì—†ë‹¤.", 0),
    ("ê³„ì•½ í•´ì§€ ì‹œ, ë‹¹ì‚¬ìëŠ” í•©ì˜ëœ ì ˆì°¨ì— ë”°ë¼ ì„œë©´ìœ¼ë¡œ í†µë³´í•´ì•¼ í•œë‹¤.", 0),
    ("ê°‘ì€ ì„ì˜ ì •ë‹¹í•œ ì‚¬ìœ  ì—†ì´ ê³„ì•½ ì¡°ê±´ì„ ì¼ë°©ì ìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ ì—†ë‹¤.", 0),
    ("ì„ì€ ê³„ì•½ ì´í–‰ ì¤‘ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ ê°‘ì—ê²Œ ì¦‰ì‹œ ë³´ê³ í•˜ê³  í˜‘ì˜í•´ì•¼ í•œë‹¤.", 0),
    ("ë³¸ ê³„ì•½ì€ ê³„ì•½ì„œì— ëª…ì‹œëœ ê¸°í•œ ë™ì•ˆ ì ìš©ë˜ë©°, ì—°ì¥ì€ ì–‘ì¸¡ í˜‘ì˜ë¥¼ í†µí•´ ì§„í–‰ëœë‹¤.", 0),
    ("ê³„ì•½ ë‹¹ì‚¬ìëŠ” ìƒí˜¸ ì¡´ì¤‘ì„ ë°”íƒ•ìœ¼ë¡œ ê³„ì•½ì„ ì´í–‰í•˜ë©°, í•„ìš” ì‹œ í˜‘ì˜ë¥¼ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.", 0),
    ("ê³„ì•½ ì¢…ë£Œ í›„ì—ë„ ë‹¹ì‚¬ìëŠ” ì¼ì • ê¸°ê°„ ë™ì•ˆ ê¸°ë°€ ìœ ì§€ ì˜ë¬´ë¥¼ ì¤€ìˆ˜í•´ì•¼ í•œë‹¤.", 0),
    ("ë³¸ ê³„ì•½ì—ì„œ ëª…ì‹œë˜ì§€ ì•Šì€ ì‚¬í•­ì€ ê´€ë ¨ ë²•ë ¹ ë° ì¼ë°˜ì ì¸ ìƒê±°ë˜ ê´€í–‰ì„ ë”°ë¥¸ë‹¤.", 0),
]

def load_trained_model(model_file):
    # âœ… ëª¨ë¸ ê°ì²´ë¥¼ ìƒˆë¡œ ìƒì„±í•œ í›„ state_dictë§Œ ë¡œë“œí•´ì•¼ í•¨
    model = BertMLPClassifier().to(device)
    model.load_state_dict(torch.load(model_file, map_location=device))
    model.eval()
    print(f"âœ… ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_file}")
    return model
loaded_model = load_trained_model(model_file)

"""
import os, sys
sys.path.append(os.path.abspath("./AI"))
import threshold_settings as ts
threshold= ts.find_threshold(loaded_model, train_loader=train_loader, val_loader=val_loader, use_train=False, device=device)
ìµœì  ì„ê³„ê°’: 0.5011
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
y_pred = []
y_true = []
threshold = 0.5011

for sentence, label in test_data:
    result = predict_toxic_clause(loaded_model,sentence,threshold)
    print(f"ğŸ“ ê³„ì•½ ì¡°í•­: {result['sentence']}")
    print(f"ğŸ” íŒë³„ ê²°ê³¼: {result['predicted_label']} (ë…ì†Œ í™•ë¥ : {result['toxic_probability']}%)")
    print(f"âœ… ì •ë‹µ: {'ë…ì†Œ' if label == 1 else 'í•©ë²•'}")
    print("-" * 50)
    y_pred.append(1 if result['toxic_probability'] >= threshold else 0)
    y_true.append(label)



# âœ… ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
roc_auc = roc_auc_score(y_true, y_pred)

# âœ… ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ğŸ“Š")
print(f"ğŸ“Œ ìµœì  ì„ê³„ê°’: {threshold:.4f}")
print(f"âœ… Accuracy: {accuracy:.4f}")
print(f"âœ… Precision: {precision:.4f}")
print(f"âœ… Recall: {recall:.4f}")
print(f"âœ… F1-score: {f1:.4f}")
print(f"âœ… ROC-AUC: {roc_auc:.4f}")