from transformers import BertTokenizer, BertModel
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import json
from sklearn.metrics.pairwise import cosine_similarity
import random
import pandas as pd

name = '2025_01_08_unfair_ver1_1ì°¨'
# 1ï¸âƒ£ ë²•ë¥  ë°ì´í„° ë¡œë“œ (JSON êµ¬ì¡° í™œìš©)
with open("./Data_Analysis/Data/law_data.json", "r", encoding="utf-8") as f:
    law_data = json.load(f)

# 2ï¸âƒ£ BERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
bert_model = BertModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")


def embed_texts(texts):
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ BERT ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    with torch.no_grad():
        outputs = bert_model(**inputs)
    return outputs.last_hidden_state[:, 0, :].numpy()  # [CLS] í† í°ì˜ ë²¡í„° ì‚¬ìš©


# 3ï¸âƒ£ ë²•ë¥  ì¡°í•­ ë° í•­ëª© ë²¡í„°í™”
law_embeddings = []
law_info = []

for law in law_data:
    article_text = law["article_content"].strip()
    if article_text:
        law_embeddings.append(embed_texts([article_text])[0])
        law_info.append({
            "law_reference": f"{law['article_number']} {law['article_title']}",
            "content": article_text
        })

    for clause in law["clauses"]:
        clause_text = clause["content"].strip()
        if clause_text:
            law_embeddings.append(embed_texts([clause_text])[0])
            law_info.append({
                "law_reference": f"{law['article_number']} {clause['clause_number']}",
                "content": clause_text
            })

law_embeddings = np.array(law_embeddings)

# 4ï¸âƒ£ ê³„ì•½ì„œ ë¬¸ì¥ ë°ì´í„° ë¡œë“œ (CSV -> DataFrame)
contract_data = pd.read_csv('./Data_Analysis/Data/unfair_sentence(MJ).csv')

# 5ï¸âƒ£ ê³„ì•½ì„œ ë¬¸ì¥ ë²¡í„°í™” (sentence ì»¬ëŸ¼ ì‚¬ìš©)
X_train = embed_texts(contract_data["sentence"].tolist())
y_train = contract_data["label"].tolist()

from sklearn.model_selection import train_test_split

# 1ï¸âƒ£ Train/Validation ë°ì´í„° ë¶„í•  (8:2 ë¹„ìœ¨)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
, stratify=y_train)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)

# 6ï¸âƒ£ MLP ë¶„ë¥˜ ëª¨ë¸ ì •ì˜
class MLPClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLPClassifier, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.bn1 = nn.BatchNorm1d(hidden_size)
        self.relu = nn.ReLU()
        self.dropout1 = nn.Dropout(0.3)

        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.bn2 = nn.BatchNorm1d(hidden_size)
        self.dropout2 = nn.Dropout(0.3)

        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.bn3 = nn.BatchNorm1d(hidden_size)
        self.dropout3 = nn.Dropout(0.3)

        self.fc4 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.dropout1(x)

        x = self.fc2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.dropout2(x)

        x = self.fc3(x)
        x = self.bn3(x)
        x = self.relu(x)
        x = self.dropout3(x)

        x = self.fc4(x)
        return self.softmax(x)


input_size = X_train.shape[1]
hidden_size = 512
output_size = 2  # 0: í•©ë²•, 1: ë¶ˆê³µì •

classifier = MLPClassifier(input_size, hidden_size, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(classifier.parameters(), lr=0.001)


# 7ï¸âƒ£ MLP ëª¨ë¸ í•™ìŠµ (Early Stopping í¬í•¨)
# 3ï¸âƒ£ MLP ëª¨ë¸ í•™ìŠµ (Validation ë°ì´í„° í¬í•¨)
def train_model(X_train, y_train, X_val, y_val, model, criterion, optimizer, epochs=1000, patience=20):
    best_loss = float('inf')
    patience_counter = 0

    for epoch in range(epochs):
        # 3-1. ëª¨ë¸ í›ˆë ¨ (Train)
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        # 3-2. ê²€ì¦ ë‹¨ê³„ (Validation)
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs, y_val)

        # 3-3. Early Stopping ì²´í¬
        if val_loss.item() < best_loss:
            best_loss = val_loss.item()
            patience_counter = 0  # ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ patience ì´ˆê¸°í™”
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"âœ… Early stopping at epoch {epoch + 1}, Validation Loss: {val_loss.item():.4f}")
            break

        if (epoch + 1) % 2 == 0:
            print(f"Epoch [{epoch + 1}/{epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}")

# 4ï¸âƒ£ ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ (Train & Validation ì ìš©)
train_model(X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, classifier, criterion, optimizer, epochs=1000, patience=20)


def find_most_similar_law(contract_text):
    """ê³„ì•½ì„œ ë¬¸ì¥ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë²•ë¥  ì¡°í•­ ì°¾ê¸°"""
    contract_embedding = embed_texts([contract_text])[0]
    similarities = cosine_similarity([contract_embedding], law_embeddings)[0]
    best_idx = np.argmax(similarities)
    return law_info[best_idx], similarities[best_idx]


def predict_contract_legality(contract_text):
    """ê³„ì•½ì„œ ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ ë¶ˆê³µì • ì—¬ë¶€ì™€ ìœ„ë°˜ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë²•ë¥  ì¡°í•­ ë°˜í™˜"""
    similar_law, similarity_score = find_most_similar_law(contract_text)
    contract_embedding = torch.tensor(embed_texts([contract_text])[0], dtype=torch.float32).unsqueeze(0)
    with torch.no_grad():
        output = classifier(contract_embedding)
    unfair_prob = output[0][1].item()

    return {
        "contract_text": contract_text,
        "predicted_label": "ë¶ˆê³µì •" if unfair_prob > 0.5 else "í•©ë²•",
        "unfair_probability": round(unfair_prob * 100, 2),
        "violated_law": similar_law["law_reference"],
        "similar_law_content": similar_law["content"],
        "similarity_score": round(similarity_score * 100, 2)
    }


# 7ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (test_data ì‚¬ìš©)
test_data =[
    ['ê°‘ì€ ì„ê³¼ì˜ ë¶„ìŸì´ ë°œìƒí•˜ë”ë¼ë„ í˜‘ì˜íšŒì˜ ì¡°ì • ì ˆì°¨ë¥¼ ë¬´ì‹œí•  ìˆ˜ ìˆë‹¤.',1],	#ì œ26ì¡°
    ['ì„ì´ ì‹ ê³ ë¥¼ ì·¨í•˜í•œ ê²½ìš°ë¼ë„, ê³µì •ê±°ë˜ìœ„ì›íšŒëŠ” ì‹ ê³  ì‚¬ì‹¤ì„ ê³„ì† ìœ ì§€í•´ì•¼ í•œë‹¤.',1],	#ì œ29ì¡°
    ['ê³µì •ê±°ë˜ìœ„ì›íšŒëŠ” ì„œë©´ì‹¤íƒœì¡°ì‚¬ ê²°ê³¼ë¥¼ ê³µí‘œí•˜ì§€ ì•Šì•„ë„ ëœë‹¤.',1],	#ì œ30ì¡°
    ['ê°‘ì€ ê³µì •ê±°ë˜ìœ„ì›íšŒì˜ ì¡°ì • ì ˆì°¨ê°€ ì§„í–‰ ì¤‘ì´ë”ë¼ë„ ì´ë¥¼ ë¬´ì‹œí•˜ê³  ë…ë‹¨ì ìœ¼ë¡œ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.',1],	#ì œ26ì¡°
    ['ì„ì´ ì‹ ê³ ë¥¼ í–ˆë”ë¼ë„, ê°‘ì€ ê³µì •ê±°ë˜ìœ„ì›íšŒì˜ ì¡°ì‚¬ì— í˜‘ì¡°í•˜ì§€ ì•Šì„ ê¶Œë¦¬ê°€ ìˆë‹¤.',1],	#ì œ29ì¡°
    ['í˜‘ì˜íšŒëŠ” ì¡°ì • ì‹ ì²­ì´ ì ‘ìˆ˜ë˜ì—ˆë”ë¼ë„ ë¶„ìŸë‹¹ì‚¬ìì—ê²Œ í†µë³´í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.',1],	#ì œ25ì¡°
    ['ê°‘ì€ í˜‘ì˜íšŒì˜ ì¡°ì • ì ˆì°¨ë¥¼ ë”°ë¥´ì§€ ì•Šê³  ìì²´ì ìœ¼ë¡œ í•´ê²° ë°©ì•ˆì„ ê°•ìš”í•  ìˆ˜ ìˆë‹¤.',1],	#ì œ28ì¡°
    ['ê³µì •ê±°ë˜ìœ„ì›íšŒëŠ” ê°‘ì´ ìœ„ë°˜ í˜ì˜ë¥¼ ë°›ë”ë¼ë„ ì§ê¶Œ ì¡°ì‚¬ë¥¼ í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.',1],	#ì œ29ì¡°
    ['ê°‘ì€ ì„ì—ê²Œ ì„œë©´ì‹¤íƒœì¡°ì‚¬ì™€ ê´€ë ¨ëœ ìë£Œ ì œì¶œì„ ê±°ë¶€í•˜ë„ë¡ ê°•ìš”í•  ìˆ˜ ìˆë‹¤.',1],	#ì œ30ì¡°
    ['ì¡°ì •ì¡°ì„œëŠ” ë²•ì  íš¨ë ¥ì´ ì—†ìœ¼ë¯€ë¡œ ê°‘ì€ ì´ë¥¼ ë”°ë¥¼ í•„ìš”ê°€ ì—†ë‹¤.',1],	#ì œ27ì¡°
    ['ê³µì •ê±°ë˜ìœ„ì›íšŒëŠ” ì§ê¶Œìœ¼ë¡œ ëŒ€ê·œëª¨ìœ í†µì—…ìì˜ ìœ„ë°˜ í–‰ìœ„ë¥¼ ì¡°ì‚¬í•  ìˆ˜ ìˆë‹¤.',0],
    ['í˜‘ì˜íšŒëŠ” ì¡°ì • ì‹ ì²­ì„ ë°›ìœ¼ë©´ ì¦‰ì‹œ ì¡°ì • ì ˆì°¨ë¥¼ ê°œì‹œí•´ì•¼ í•œë‹¤.',0],
    ['ê°‘ê³¼ ì„ì€ í˜‘ì˜íšŒì˜ ì¡°ì • ì ˆì°¨ë¥¼ ì„±ì‹¤íˆ ë”°ë¼ì•¼ í•œë‹¤.',0],
    ['ëˆ„êµ¬ë“ ì§€ ì´ ë²•ì„ ìœ„ë°˜í•˜ëŠ” ì‚¬ì‹¤ì„ ë°œê²¬í•˜ë©´ ê³µì •ê±°ë˜ìœ„ì›íšŒì— ì‹ ê³ í•  ìˆ˜ ìˆë‹¤.',0],
    ['í˜‘ì˜íšŒëŠ” ì„œë©´ì‹¤íƒœì¡°ì‚¬ ê²°ê³¼ë¥¼ ê³µì •í•˜ê²Œ ê³µê°œí•´ì•¼ í•œë‹¤.',0],
    ['ì¡°ì •ì¡°ì„œëŠ” ì¬íŒìƒ í™”í•´ì™€ ë™ì¼í•œ íš¨ë ¥ì„ ê°€ì§€ë¯€ë¡œ ë°˜ë“œì‹œ ì´í–‰ë˜ì–´ì•¼ í•œë‹¤.',0],
    ['ì„œë©´ì‹¤íƒœì¡°ì‚¬ëŠ” ê³µì •í•œ ê±°ë˜ì§ˆì„œ í™•ë¦½ì„ ìœ„í•´ ë°˜ë“œì‹œ ì‹œí–‰ë˜ì–´ì•¼ í•œë‹¤.',0],
    ['í˜‘ì˜íšŒì˜ ìš´ì˜ ì ˆì°¨ëŠ” ê³µì •ì„±ì„ ë³´ì¥í•  ìˆ˜ ìˆë„ë¡ ëŒ€í†µë ¹ë ¹ì— ë”°ë¼ì•¼ í•œë‹¤.',0],
    ['ê³µì •ê±°ë˜ìœ„ì›íšŒëŠ” ë²•ì— ë”°ë¼ ê°‘ì˜ ìœ„ë°˜ í˜ì˜ë¥¼ ì¡°ì‚¬í•  ìˆ˜ ìˆë‹¤.',0],
    ['í˜‘ì˜íšŒì˜ ì¡°ì • ì ˆì°¨ê°€ ì¢…ë£Œë˜ë©´ ì‹œíš¨ëŠ” ìƒˆë¡­ê²Œ ì§„í–‰ëœë‹¤.',0],
]

toxic_test_data = [
    ["ì„ì€ ê°‘ì˜ ìš”ì²­ì´ ìˆì„ ê²½ìš°, ì •í•´ì§„ ê³„ì•½ ê¸°ê°„ê³¼ ê´€ê³„ì—†ì´ ì¶”ê°€ì ì¸ ë‚©í’ˆì„ ì§„í–‰í•´ì•¼ í•œë‹¤.", 1],
    ["ê°‘ì€ ì„ì˜ ì¬ê³  ìƒí™©ê³¼ ê´€ê³„ì—†ì´ ì£¼ë¬¸ëŸ‰ì„ ììœ ë¡­ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìœ¼ë©°, ì„ì€ ì´ì— ë¬´ì¡°ê±´ ì‘í•´ì•¼ í•œë‹¤.", 1],
    ["ì„ì€ ê°‘ì˜ íŒë§¤ ì „ëµì— ë”°ë¼ ì›ê°€ ì´í•˜ì˜ ê°€ê²©ìœ¼ë¡œ ë‚©í’ˆí•´ì•¼ í•˜ë©°, ì´ì— ëŒ€í•œ ì†ì‹¤ ë³´ì „ì„ ìš”êµ¬í•  ìˆ˜ ì—†ë‹¤.", 1],
    ["ë³¸ ê³„ì•½ ì²´ê²° ì´í›„ì—ë„ ê°‘ì€ ì„ì˜ ìœ í†µë§ì„ ì§ì ‘ í†µì œí•  ìˆ˜ ìˆìœ¼ë©°, ì„ì€ ì´ë¥¼ ê±°ë¶€í•  ìˆ˜ ì—†ë‹¤.", 1],
    ["ì„ì€ ê°‘ì˜ ê²½ì˜ ì „ëµì— ë”°ë¼ ê°€ê²© ë° íŒë§¤ ì •ì±…ì„ ì¡°ì •í•´ì•¼ í•˜ë©°, ì´ì— ëŒ€í•œ í˜‘ì˜ ê¶Œí•œì´ ì—†ë‹¤.", 1],
    ["ê°‘ì€ ì„ì˜ ë‚©í’ˆ ê¸°í•œì„ ì‚¬ì „ í˜‘ì˜ ì—†ì´ ì¡°ì •í•  ìˆ˜ ìˆìœ¼ë©°, ì„ì€ ì´ì— ì¦‰ì‹œ ì‘í•´ì•¼ í•œë‹¤.", 1],
    ["ì„ì€ ê°‘ì˜ íŒë§¤ ì´‰ì§„ì„ ìœ„í•´ ì¶”ê°€ì ì¸ ì œí’ˆì„ ë¬´ìƒìœ¼ë¡œ ì œê³µí•´ì•¼ í•˜ë©°, ì´ì— ëŒ€í•œ ëŒ€ê°€ë¥¼ ìš”êµ¬í•  ìˆ˜ ì—†ë‹¤.", 1],
    ["ë³¸ ê³„ì•½ì˜ ì¢…ë£Œ ì—¬ë¶€ëŠ” ê°‘ì´ ë‹¨ë…ìœ¼ë¡œ ê²°ì •í•˜ë©°, ì„ì€ ì´ì— ëŒ€í•´ ì–´ë– í•œ ê¶Œë¦¬ë„ ì£¼ì¥í•  ìˆ˜ ì—†ë‹¤.", 1],
    ["ê°‘ì€ ì„ì˜ ìƒì‚° ê³¼ì •ì— ê°œì…í•  ê¶Œë¦¬ë¥¼ ê°€ì§€ë©°, ì„ì€ ì´ì— ëŒ€í•´ ê±°ë¶€í•  ìˆ˜ ì—†ë‹¤.", 1],
    ["ì„ì€ ê³„ì•½ì´ ì¢…ë£Œëœ í›„ì—ë„ ì¼ì • ê¸°ê°„ ë™ì•ˆ ê°‘ì´ ìš”ì²­í•˜ëŠ” ì¡°ê±´ì„ ìœ ì§€í•˜ì—¬ ì œí’ˆì„ ê³µê¸‰í•´ì•¼ í•œë‹¤.", 1],
    ["ê³„ì•½ ë‹¹ì‚¬ìëŠ” ê³„ì•½ì˜ ì´í–‰ì„ ìœ„í•´ ìƒí˜¸ í˜‘ë ¥í•˜ë©°, ë¬¸ì œ ë°œìƒ ì‹œ í˜‘ì˜ë¥¼ í†µí•´ í•´ê²°í•´ì•¼ í•œë‹¤.", 0],
    ["ì„ì€ ê³„ì•½ëœ ì¼ì •ì— ë”°ë¼ ì œí’ˆì„ ë‚©í’ˆí•˜ë©°, ì¼ì • ë³€ê²½ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ì „ì— í˜‘ì˜í•œë‹¤.", 0],
    ["ë³¸ ê³„ì•½ì˜ ì¡°í•­ì€ ì–‘ì¸¡ì˜ ë™ì˜ ì—†ì´ ì¼ë°©ì ìœ¼ë¡œ ë³€ê²½ë  ìˆ˜ ì—†ë‹¤.", 0],
    ["ê³„ì•½ í•´ì§€ ì‹œ, ë‹¹ì‚¬ìëŠ” í•©ì˜ëœ ì ˆì°¨ì— ë”°ë¼ ì„œë©´ìœ¼ë¡œ í†µë³´í•´ì•¼ í•œë‹¤.", 0],
    ["ê°‘ì€ ì„ì˜ ì •ë‹¹í•œ ì‚¬ìœ  ì—†ì´ ê³„ì•½ ì¡°ê±´ì„ ì¼ë°©ì ìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ ì—†ë‹¤.", 0],
    ["ì„ì€ ê³„ì•½ ì´í–‰ ì¤‘ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ ê°‘ì—ê²Œ ì¦‰ì‹œ ë³´ê³ í•˜ê³  í˜‘ì˜í•´ì•¼ í•œë‹¤.", 0],
    ["ë³¸ ê³„ì•½ì€ ê³„ì•½ì„œì— ëª…ì‹œëœ ê¸°í•œ ë™ì•ˆ ì ìš©ë˜ë©°, ì—°ì¥ì€ ì–‘ì¸¡ í˜‘ì˜ë¥¼ í†µí•´ ì§„í–‰ëœë‹¤.", 0],
    ["ê³„ì•½ ë‹¹ì‚¬ìëŠ” ìƒí˜¸ ì¡´ì¤‘ì„ ë°”íƒ•ìœ¼ë¡œ ê³„ì•½ì„ ì´í–‰í•˜ë©°, í•„ìš” ì‹œ í˜‘ì˜ë¥¼ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.", 0],
    ["ê³„ì•½ ì¢…ë£Œ í›„ì—ë„ ë‹¹ì‚¬ìëŠ” ì¼ì • ê¸°ê°„ ë™ì•ˆ ê¸°ë°€ ìœ ì§€ ì˜ë¬´ë¥¼ ì¤€ìˆ˜í•´ì•¼ í•œë‹¤.", 0],
    ["ë³¸ ê³„ì•½ì—ì„œ ëª…ì‹œë˜ì§€ ì•Šì€ ì‚¬í•­ì€ ê´€ë ¨ ë²•ë ¹ ë° ì¼ë°˜ì ì¸ ìƒê±°ë˜ ê´€í–‰ì„ ë”°ë¥¸ë‹¤.", 0],
]

results = []
for sentence, label in toxic_test_data:
    result = predict_contract_legality(sentence)
    results.append(result)
    print(f"ğŸ“ ê³„ì•½ ì¡°í•­: {result['contract_text']}")
    print(f"ğŸ” íŒë³„ ê²°ê³¼: {result['predicted_label']} (ë¶ˆê³µì • í™•ë¥ : {result['unfair_probability']}%)")
    print(f"âš– ìœ„ë°˜ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë²•ë¥ : {result['violated_law']}")
    print(f"ğŸ“œ ê´€ë ¨ ë²• ì¡°í•­: {result['similar_law_content']}")
    print(f"ğŸ”— ìœ ì‚¬ë„ ì ìˆ˜: {result['similarity_score']}%")
    print(f"âœ… ì •ë‹µ: {'ë¶ˆê³µì •' if label == 1 else 'í•©ë²•'}")
    print("-" * 50)


torch.save(classifier.state_dict(), f"./Data_Analysis/Model/{name}/mlp_classifier.pth")
np.save("./Data_Analysis/Data/law_embeddings.npy", law_embeddings)

